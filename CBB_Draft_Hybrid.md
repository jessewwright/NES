title: "Testing Architectural Necessity in Cognitive Theory: A Simulation-Based Framework Using Normative Decision Making"
author:
Jesse Wright
date: "June 4, 2025"
bibliography: references.bib
link-citations: true
Abstract
Is normative influence an architecturally distinct decision mechanism or an emergent byproduct of other cognitive processes? Here we present a simulation-based framework to test such questions of architectural necessity in cognitive theory. As a case study, we introduce the Normative Executive System (NES), a minimal extension of the Drift Diffusion Model that includes an explicit norm weight parameter ($w_n$) to represent normative influence. In NES, $w_n$ modulates the drift rate in opposition to stimulus salience, allowing formal dissociation between norm-driven and utility-driven decision dynamics. Using Simulation-Based Calibration (SBC) with two inference methods (Approximate Bayesian Computation with Sequential Monte Carlo and Neural Posterior Estimation), we demonstrate that $w_n$ is statistically identifiable from simulated choice/response time data under realistic conditions. Furthermore, standard hierarchical DDMs (even when augmented with conflict-level regressors) fail to recover $w_n$ from NES-generated data, indicating a fundamental architectural mismatch. NES also produces distinctive behavioral signatures (e.g. conflict-conditioned reductions in errors and response times) that conventional DDM parameters cannot replicate. Together, these findings provide simulation-based evidence that introducing an explicit normative mechanism improves model identifiability and predictive specificity. More broadly, this work offers a generalizable framework for rigorously evaluating when new cognitive mechanisms are warranted, using norm-guided decision-making as a proof-of-concept example. We discuss implications for building parsimonious yet testable cognitive theories and for establishing higher standards of model validation in cognitive science. Keywords: Normative Executive System; Drift Diffusion Model; Simulation-Based Calibration; Norm Weight; Moral Cognition; Decision Architecture
Introduction
Across many domains of cognitive science, theorists grapple with whether certain behavioral influences reflect necessary components of the mind’s architecture or merely emergent effects of more general processes. This question carries high stakes for theory development: positing a specialized decision mechanism can enrich a model’s explanatory power, but it also complicates the architecture and must be justified by clear evidence. In practice, distinguishing a truly distinct process from a clever re-tuning of existing parameters is challenging. Standard model-fitting techniques often accommodate new data by adjusting generic parameters, potentially masking the presence or absence of a dedicated mechanism. Thus, methodological innovation is needed to rigorously test when an influence warrants treatment as a fundamental element of the cognitive architecture. One domain where this issue arises acutely is moral and normative decision-making. People often make norm-consistent choices even when self-interest, emotion, and learned rewards all favor doing otherwise. For example, we stop at red lights on empty roads, keep unenforceable promises, and help strangers at personal cost. Such behaviors persist in the absence of material incentives or direct social enforcement, hinting at an internal factor beyond simple utility or habit. Classical models usually explain norm adherence by folding it into emergent processes within general-purpose decision architectures. For instance, utility-based models might treat norms as just another high-value preference, reinforcement learning accounts attribute norm-following to conditioned social feedback, dual-process theories invoke generic cognitive control or emotion-regulation to override selfish impulses, and conflict-monitoring frameworks see norm compliance as a domain-general inhibitory control response (Bello & Malle, 2023; Cushman, 2015). In all these approaches, norms are not granted a unique status; rather, norm-following is expected to arise “for free” as a byproduct of existing valuation, learning, or control parameters. However, many moral choices appear uncued, unsupervised, and divorced from self-interest, suggesting that something more may be at play (Cushman, 2015). For example, one might reject an unfair offer in an economic game even with no future repercussions, or children might follow invented rules in games with no reward, which is difficult to attribute entirely to learned utility or fear of sanction. These observations motivate a bold theoretical question: Could normative influence itself be a necessary component of the decision architecture? [@Greene2001fMRI] In other words, is there a dedicated cognitive mechanism that drives norm-adherent behavior, independent of and irreducible to other decision-making processes? For theoretical progress, one must balance parsimony and explanatory power. Unnecessary mechanisms violate Occam’s razor, but genuinely distinct processes require explicit representation to ensure identifiability and replicability. We advocate for requiring rigorous evidence of a mechanism’s necessity before elevating it to a fundamental component of the cognitive architecture. Despite this need, direct empirical tests of “architecturally necessary vs. emergent” status are rare. In the realm of moral decision-making, recent reviews have highlighted the lack of formal, model-based tests for whether explicit representations of norms are necessary to explain behavior [@Hare2009SelfControl]. Researchers have typically inferred the role of norms indirectly (e.g., via questionnaires or post hoc model fits) rather than building models where norm influence is a separate, measurable factor. Without such tests, debates over domain-specific mechanisms often remain stuck at the level of plausible interpretations or statistical model comparisons, which can be inconclusive. Standard goodness-of-fit or likelihood ratio tests are insufficient because a sufficiently flexible generalist model might fit the data nearly as well as a specific model, obscuring the unique contribution of the new mechanism. What is needed is a more stringent simulation-based methodology that can ask: if the world did have a dedicated normative influence (or other candidate mechanism), could we detect it? And conversely, would a model lacking that mechanism systematically falter in capturing the data?
Case Study: Normative Influence in Decision Making
To illustrate our framework, we designed a minimal model called the Normative Executive System (NES), which extends the drift-diffusion model by adding a single parameter for norm influence. NES’s drift rate $v$ is defined to include two competing components—stimulus-driven salience versus normative pressure—combined according to a context variable $\lambda$ that represents the degree of norm–utility conflict. Formally: 
v
=
w
s
(
1
−
λ
)
  
−
  
w
n
 
λ
,
v=w 
s
​
 (1−λ)−w 
n
​
 λ, where $w_s$ is the usual salience weight (capturing evidence favoring the more “incentivized” option) and $w_n$ is the norm weight representing internalized normative pressure. The factor $\lambda \in [0,1]$ indexes conflict between the two: $\lambda=0$ means no normative conflict (the decision is purely driven by utility/salience), while $\lambda=1$ means maximal conflict (the norm completely opposes the salient choice). A larger $w_n$ causes the drift to tilt more toward the norm-favored choice as $\lambda$ increases. NES retains the standard DDM parameters for decision threshold ($a$) and non-decision time ($t_0$), which we held constant in simulations to isolate the effects of $w_n$ and $w_s$. Task Design: We simulated a binary decision task with varying levels of norm conflict. Each trial was assigned a conflict value $\lambda$ (e.g., $0.0, 0.25, 0.5, 0.75, 1.0$). At $\lambda=0$, norm and self-interest align, making the decision straightforward; at $\lambda=1$, following the norm directly opposes a strong self-interested incentive. Intermediate $\lambda$ values create graduated levels of tension between doing what’s right versus what’s rewarding. We generated synthetic datasets for multiple simulated individuals (with trials at each $\lambda$ level) and used these data to evaluate whether $w_n$ could be identified and whether its absence caused systematic model failures, as described next.
Testing the Framework: Identifiability Analysis
ABC-SMC Parameter Recovery: In our first set of simulations, we generated 100 synthetic datasets from NES with random “true” parameter values (including $w_n$ drawn from a broad range) and applied ABC-SMC inference to each. The recovered $w_n$ values closely matched the true values (Pearson $r\approx0.9$), and the posterior credible intervals contained the true $w_n$ about 95% of the time. Moreover, simulation-based calibration (SBC) showed nearly uniform rank histograms for $w_n$, indicating no systematic bias in recovery. Figure 1 (Appendix) illustrates an example rank histogram for $w_n$, confirming that $w_n$ is identifiable under realistic conditions. We also verified that identifiability was robust to analysis variations. For instance, using different summary statistics or weighting schemes in ABC still yielded uniform rank statistics for $w_n$. This increases our confidence that the positive recovery of $w_n$ is not an artifact of a particular inference setup, but rather reflects an inherent signal in the data. Neural Posterior Estimation (NPE) Results: Using a neural inference method [@sbi_package_tejero_etal_2020], we obtained similar results. After training a neural posterior estimator on a large simulation set, we inferred parameters on 100 new datasets. The NPE recovered $w_n$ accurately: posteriors for $w_n$ were sharply peaked near the true values, and SBC ranks were again uniform (no calibration deviations; see Figure 2 in Appendix). Notably, NPE could jointly recover all parameters ($w_n$, $w_s$, $a$, $t_0$) without confusion, as $w_n$ showed little correlation with other parameters. The convergence of ABC and NPE results strengthens the evidence that $w_n$ is a genuine, recoverable feature of the data rather than a “ghost” parameter. Hierarchical DDM (HDDM) Mismatch Analysis: Next, we asked whether a model lacking $w_n$ could fit data generated by NES. We fit a hierarchical DDM (HDDM) to the same simulated datasets, allowing its drift rate to vary by conflict level (to give the DDM a chance to mimic norm effects via extra flexibility). Despite this, the HDDM failed to recapitulate the true norm influence. The correlation between the HDDM’s fitted drift-vs-conflict slopes and the true $w_n$ values was low (e.g., $r\approx0.4$ in one analysis), indicating it could not reliably recover the norm effect. The HDDM consistently underestimated the impact of high conflict on drift, and often the true $w_n$ lay at the extreme tails of the HDDM’s posterior—meaning the HDDM was confidently wrong about norm influence. The core problem was that the HDDM lacked a dedicated place for normative influence, so it misattributed the norm-driven effects to other parameters (or noise). In high-conflict trials especially, the HDDM’s drift estimates were far off (e.g., 30–50% too low in some conditions) because the model could not represent the internal tug-of-war between norm and salience. In short, a model without an explicit $w_n$ could not “fake” data generated with a norm mechanism, supporting the claim that $w_n$ is a structurally necessary component for capturing those patterns.
Unique Behavioral Signatures
Beyond statistical identifiability, an architecturally necessary mechanism should produce behavioral patterns that other models cannot. NES indeed predicts a distinctive pattern under high normative influence. Counter-intuitively, when $w_n$ is large, increasing conflict can lead to faster and more accurate decisions (norm-consistent ones). This is opposite to the usual pattern where more conflict (or difficulty) slows responses and increases errors. In NES, a strong norm weight drives the decision process toward the norm-adherent choice even as conflict rises, sometimes even accelerating the decision once the norm “takes over.” For example, with $w_n$ high, the model makes fewer errors at $\lambda=1.0$ (max conflict) than at moderate conflict, and reaction times that were initially slowed by conflict start to speed up again at the highest conflict level. By contrast, when $w_n\approx0$ (no norm influence), NES reduces to a standard DDM and shows the expected slowing and accuracy decline as conflict increases. Figure 3 illustrates this contrast: with $w_n=1.5$, accuracy actually improves at high conflict and mean RT drops after an initial increase, whereas with $w_n=0$ accuracy falls and RT rises monotonically with conflict. Crucially, we found that no configuration of a norm-less DDM could reproduce this joint pattern of speed and accuracy. Adjusting the decision threshold or drift rate in a standard DDM could either reduce errors or speed up responses, but not achieve the combination that NES generates (fewer errors and faster responses specifically at high conflict). For instance, raising the threshold might decrease errors but at the cost of much slower responses across all conditions, unlike the selective speeding we see with a high $w_n$. Lowering $w_s$ (salience) uniformly degrades performance rather than producing an improvement at high conflict. We systematically tried such adjustments and none yielded the “paradoxical” performance profile that a non-zero $w_n$ produces. Figure 5 illustrates this point: only the model with an explicit norm weight lies in the region of performance space characterized by decreasing error rates and non-increasing RTs as conflict grows. All other models (including various threshold or drift adjustments) occupy a different region (higher error slopes and/or RT slopes). This analysis confirms that the behavioral effects of $w_n$ are qualitatively unique. Empirically, this unique signature offers a way to test for normative mechanisms in people: if some individuals actually show faster, more accurate choices under increased normative conflict, it would be hard to explain without a dedicated norm process. If no one shows such a pattern, then positing a special norm mechanism may be unnecessary. In our simulations, the signature becomes pronounced at sufficiently high $w_n$; moderate values yield intermediate patterns. The key takeaway is that NES’s norm parameter is not just mathematically identifiable, but behaviorally consequential in a way that defies mimicry by other parameters.
Empirical Stress Test: Fitting NES to Human Framing Choices
As a final demonstration, we applied NES to human decision data to evaluate its relevance beyond simulations. We analyzed choices and response times from a moral decision-making task where the context framing modulated normative conflict (similar to paradigms in [@Gautheron2024conflictinmoral]). For example, in one condition an option was presented with an explicit moral rule attached (high norm salience), while in another it was framed neutrally without invoking norms (low norm salience). This manipulation created variation in internal normative pressure on otherwise similar decisions, providing an opportunity to test whether NES can capture framing-induced differences. Using a hierarchical Bayesian approach, we fit the NES model to participants’ data across the two framing conditions. Including the norm parameter $w_n$ markedly improved the model’s ability to explain the choices and RTs compared to a standard DDM without $w_n$. The posterior estimates for $w_n$ were consistently above zero for many individuals, indicating a measurable influence of normative framing on their decision dynamics. In contrast, the baseline DDM systematically underpredicted the differences between framing conditions – for instance, it could not account for slower but more norm-consistent choices in the moral-frame condition. This result suggests that a non-zero $w_n$ was necessary to capture the behavioral shift caused by moral framing, aligning with the idea that an explicit norm influence contributes to human decisions. This empirical stress test also highlighted some limitations. The benefit of including $w_n$, though evident, was moderate in magnitude and varied across individuals. Some participants’ choices were well explained without invoking a norm mechanism, implying substantial individual differences in normative sensitivity. Moreover, human data carry noise and contextual complexities that our simple model does not address. Thus, while the NES fit offered initial support for a distinct norm factor, it also underscores the challenge of conclusively validating new mechanisms using behavioral data alone. Nevertheless, this exercise illustrates how our framework can be extended to real-world decisions and demonstrates that a dedicated normative parameter can capture unique variance in human choice patterns. 
 Figure 4: Model fitting results for the framing task. Including the normative weight parameter ($w_n$) allows NES to better capture the effect of moral framing on decisions, whereas a standard DDM without $w_n$ cannot. The model with $w_n$ more closely reproduces the higher norm-consistent choice proportion and slower response times observed under the moral-frame condition.
Implications for Theory Building
Our findings also emphasize how higher standards can sharpen theoretical debates. When proposing a new cognitive mechanism, researchers should not rely solely on arguments that existing models cannot explain a phenomenon; they should also provide simulation-based evidence that the new mechanism is identifiable and produces unique predictions that simpler models cannot. By expecting such evidence, the field can ensure that added model complexity is justified by a true gain in explanatory power, leading to more parsimonious yet rigorous theories. Furthermore, our case study challenges the assumption that complex behaviors always “emerge” from simpler processes. The norm-following patterns we examined could not be replicated by a model lacking an explicit norm parameter, which suggests that normative influence may require its own dedicated representation rather than being reducible to general decision mechanisms. In other domains, our framework could be applied similarly to test, for example, whether specialized social-cognitive modules or language faculties are necessary or if general processes suffice. This shifts the dialogue from plausibility to empirical demonstrability of putative mechanisms. Limitations: Despite the strengths of our approach, several limitations must be acknowledged. First, our analyses were entirely based on simulated data. While this was appropriate for testing identifiability and theoretical distinctions, evidence for NES’s advantage on actual human behavior remains preliminary. In fact, our initial test fitting NES to human choices (Appendix B) showed that including $w_n$ improved fit only modestly. Real decisions involve noise and unmodeled factors, so detecting a new mechanism from behavior alone may require larger studies or additional evidence. Second, the scope of the model is limited. NES as implemented covers binary choice with a static norm conflict parameter. Real-world normative decisions are often more complex: multi-alternative choices, dynamic norm learning over time, etc. Currently, $w_n$ in our model is treated as a stable influence within a session. In reality, people may adjust their norm adherence based on context or reinforcement (perhaps learning when to enforce a norm or not). Thus, extending the model to include learning dynamics for $w_n$ (e.g., updating norm weight through experience or feedback) would increase its realism [@Hare2009SelfControl]. Third, our model does not incorporate any neuroscientific constraints – it’s a purely algorithmic description. It remains to be tested whether there are neural signals corresponding to $w_n$ or if neural network models can evolve a similar parameter. We assume homogeneity in how $w_n$ operates across trials and individuals; individual differences in norm adherence (cultural, personality-driven, etc.) would be interesting to explore by making $w_n$ subject-specific or hierarchical (Bello & Malle, 2023; Cushman, 2015). In our simulations, we saw that recovery at the individual level is possible; the next step would be to simulate heterogeneous groups and see if group-level inference correctly identifies the distribution of $w_n$. Finally, the generalizability of our specific results should be taken cautiously. Normative influence is just one candidate for an architecturally distinct and necessary mechanism. We believe our framework is general, but each new application may face its own challenges. There might be cases where simulation-based testing reveals that what was thought to be a new mechanism is actually not identifiable, and thus perhaps not a separate mechanism after all. Such null results are as important as positive ones, as they can save the field from chasing ghosts.
Conclusion
In summary, we present a systematic simulation-based framework for assessing when a proposed cognitive mechanism is truly architecturally necessary, using normative decision-making as an illustrative test case. By using simulation-based methods with an illustrative model (NES) in the domain of normative decision-making, we showed how one can gather evidence that a mechanism is (i) statistically identifiable, (ii) associated with unique behavioral patterns, and (iii) indispensable in the sense that models lacking it fail to account for the data. In doing so, we found that explicitly modeling normative influence with a dedicated parameter ($w_n$) yields all three of these hallmarks: the norm parameter can be recovered from behavior with well-calibrated accuracy, it generates distinctive effects on choice and response time that standard models cannot replicate, and those standard models systematically misestimate data that include normative effects. These results strongly suggest that normative influence operates as a mechanistically distinct factor in decision dynamics, lending computational credence to the idea that humans have an internal normative guidance process rather than norms being just implicit in other processes. More generally, our study demonstrates the value of moving beyond intuitive arguments and model fits to simulation-based theory testing. By raising the bar for what counts as evidence for a new theoretical construct, we can achieve more robust and transparent cognitive theories. We have treated the NES model as a proof-of-concept; the broader contribution is the framework and methodology that can be applied to any theory positing a new component. We envision that future work on cognitive architectures – whether in moral psychology, reinforcement learning, memory systems, or language – will benefit from adopting similar practices: explicitly modeling the hypothesized mechanism, testing its identifiability, and checking that its predicted signatures are unique. Embracing these practices will improve the rigor and falsifiability of cognitive theories, ultimately leading to a deeper understanding of which aspects of the mind are fundamental and which emerge from combinations of simpler elements. In conclusion, by marrying theoretical insight with simulation-based rigor, we provided a template for how to test architectural assumptions in cognitive models. Normative decision-making served as a compelling test case, revealing the potential necessity of a dedicated norm module. As cognitive science continues to evolve, approaches like this will be instrumental in charting the architecture of the mind – distinguishing the true building blocks from epiphenomena and ensuring that our models of cognition are both parsimonious and true to the complexities of behavior.
Appendices
Appendix A: Simulation and Inference Details
Task Simulation and Fixed Parameters: All simulations were conducted with a drift-diffusion process as described for NES. We used a constant diffusion noise $\sigma = 1.0$ (standard deviation of evidence noise per $\sqrt{\text{s}}$) and integrated the stochastic differential equation using the Euler–Maruyama method with a time step $dt = 0.01$ s. Each simulated trial ran until either a decision boundary was crossed or a maximum time $T_{\max} = 10$ s was reached (responses that did not occur by $T_{\max}$ were treated as non-decisions or omitted, though this was rare with the chosen parameters). In most simulations, we fixed certain DDM parameters to reduce complexity: the decision threshold was set to $a = 1.0$ (or sometimes 1.5) and the non-decision time to $t_0 = 0.2$ s, values in line with typical human data. The salience weight $w_s$ was also often fixed (e.g., $w_s = 1.0$) to focus on recovering $w_n$. We verified through additional simulations that small variations in these fixed parameters (±10% jitter) did not qualitatively affect the identifiability of $w_n$ or the presence of unique signatures (Bello & Malle, 2023). Using fixed $a$ and $t_0$ effectively assumes that normative influence primarily manifests through drift adjustments; this assumption was part of our minimal model design. Future hierarchical versions could relax this, but for our purposes it aided in highlighting $w_n$’s effects. Each synthetic dataset typically included 5 simulated subjects, each with 1000 trials (200 per each of 5 conflict levels). These choices were motivated by a balance between computational tractability and realistic experimental sizes. We note that identifiability of $w_n$ was observed even with somewhat fewer trials (e.g. 500 per subject), though with higher variance; having ~1000 trials/subject gave more stable estimates. Approximate Bayesian Computation (ABC): We implemented ABC with Sequential Monte Carlo using the pyABC library [@Klinger2018pyABC]. For each dataset, the ABC procedure was as follows: We defined summary statistics capturing key features of the data: namely, for each conflict level $\lambda$, the mean correct RT, mean error RT, accuracy (proportion correct), and perhaps higher moments (variance of RT) – in total a vector of summary statistics encompassing the speed-accuracy profile across conditions. We then defined a distance function $\rho$ between simulated and observed summary vectors (e.g., a weighted Euclidean distance). We experimented with different weighting schemes for $\rho$: equal weights (all stats equally important), mild skew (slightly more weight to accuracy than RT, for instance), strong skew (heavily weighting one type of statistic), and exclusively RT-focused or choice-focused weightings. The idea was to ensure our conclusions didn’t depend on a particular choice of summary emphasis. The population size for ABC-SMC was on the order of 1000 particles, and we ran it for multiple generations, tightening the distance threshold each time (the final tolerance was chosen such that a reasonable number of particles – a few tens – were accepted). Priors for parameters were uniform over broad ranges (e.g., $w_n \sim \text{Uniform}(0, 3)$, $w_s \sim \text{Uniform}(0, 2)$, etc., encompassing the true values). We also validated the ABC pipeline on data generated from a standard DDM (with $w_n=0$) to ensure it did not falsely indicate identifiability where none should exist. In all tested configurations, ABC produced posterior distributions for $w_n$ centered near the true value. The SBC analysis aggregated 100 such ABC inferences (each on data from random draws of true parameters). As reported, none of the examined summary weighting schemes showed a deviation from uniform in the rank test – confirming that our ABC approach was robust to the choice of summary importance (Bello & Malle, 2023). The acceptance rates and convergence diagnostics from pyABC indicated that the algorithm had adequately sampled the relevant parameter space. Neural Posterior Estimation (NPE): We utilized the sbi toolkit’s implementation of Sequential Neural Posterior Estimation (also known as SNPE, specifically the C variant which includes conditioning on simulations) [@sbi_package_tejero_etal_2020]. The neural network used was a Masked Autoregressive Flow (MAF), which is a type of normalizing flow allowing flexible approximation of complex posteriors. Table A1 below summarizes the architecture and training hyperparameters we employed for NPE:
Component	Specification
Network Architecture	5-layer Masked Autoregressive Flow (MAF)
Hidden Units per layer	[50, 50] (two hidden layers of 50 each in the MADE networks)
Training Simulations	10,000 parameter sets drawn from priors (with one simulated dataset per set)
Training Batch Size	50
Optimizer & Learning Rate	Adam optimizer, learning rate $1 \times 10^{-4}$
Training Epochs	~120 (with early stopping on validation loss)
Posterior Samples	1,000 per simulation (for SBC rank computation)
Parameters Inferred	$w_n$, $w_s$, $a$, $t_0$ (jointly)
Table A1. NPE neural network and training details. The prior distributions for NPE were the same as used in ABC for comparability (uniform broad ranges for each parameter). We included $a$ and $t_0$ in the inference to see if NPE could handle the joint estimation—indeed it did, although including extra parameters increased training time slightly. After training on the 10k simulations, the NPE was applied to new datasets (100 held-out simulations for SBC). The posterior produced by NPE for each dataset was represented by drawing 1000 samples from the MAF. We then computed ranks by comparing each true parameter to the sorted list of its 1000 posterior samples. As mentioned in the main text, all parameters had approximately uniform rank distributions; $w_n$ in particular showed no bias (e.g., true $w_n$ landed below the median about 50% of the time across iterations, as expected). One nuance: $w_s$ and $t_0$ had slightly larger variance in ranks, but still within confidence intervals for uniformity, whereas $w_n$ and $a$ were almost perfectly uniform (Bello & Malle, 2023). This likely reflects that $w_n$ and $a$ have very distinct effects on the data (one affects conflict-dependent drift, the other overall speed-accuracy tradeoff), making them easiest to infer, while $w_s$ and $t_0$ have more subtle or less condition-dependent influences, making inference a tad noisier. Nonetheless, there was no systematic bias for any parameter, indicating successful joint recovery. Model Implementation and Code: All code for simulating the NES model and performing inference is written in Python (v3.10). The simulation engine uses NumPy for vectorized operations on the drift-diffusion process. For the HDDM analysis, we used the HDDM Python package (which relies on PyMC under the hood for MCMC sampling). We verify in a preliminary check that our HDDM setup can recover known parameters from data simulated by a standard DDM (as shown in Figure 1 of the main text, which was a sanity check with $w_n=0$) (Bello & Malle, 2023). All experiments were run on a workstation with 40 GB RAM and an NVIDIA RTX-series GPU, which accelerated the neural network training for NPE (ABC-SMC and HDDM sampling were run on CPU) [@sbi_package_tejero_etal_2020]. The entire pipeline (simulation -> ABC/NPE -> analysis) has been made available in a publicly accessible repository for reproducibility.
Appendix B: Human Decision-Making Task Details
We provide an overview of the empirical task used in the "Empirical Stress Test" section. Participants made decisions in two conditions that varied the framing of a moral choice. Table B1 summarizes the key features of these conditions.
Condition	Description	Norm Conflict
Neutral Frame	Decision presented without moral language, focusing on personal outcomes only. For example, a choice described purely in terms of individual gain or loss, with no mention of ethical principles.	Low
Moral Frame	Decision framed with explicit moral context or rule, highlighting normative implications. The same choice scenario is introduced with a moral duty or principle (e.g., keeping a promise or helping someone in need) to emphasize the ethical stakes.	High
Table B1: Summary of conditions in the moral framing task. All participants experienced both framing conditions (within-subjects), with multiple trials per condition (e.g., 20 trials each). The moral frame was intended to induce a stronger normative influence on choice, whereas the neutral frame provided a baseline with minimal norm emphasis.
Appendix C: Additional Analysis and Figures
HDDM Recovery Metrics: Table 1 in the main text provided a snapshot of the HDDM performance in one scenario. For completeness, here we note that we ran 20 independent recovery trials of the HDDM on NES-generated data. In each, we drew a new true $w_n$ (uniform 0.1–2.0) and simulated 5 subjects × 1000 trials with that $w_n$ (and fixed $w_s, a, t_0$). We then fit the hierarchical DDM. The median Pearson correlation between true and fitted per-condition drift rates across those runs was about 0.50 (with a range roughly 0.3 to 0.7 depending on the random sample). The slope (implied $w_n$) recovery correlation was lower, median about 0.4, echoing the particular example we highlighted. In the majority of runs, the HDDM’s 95% highest-density interval for the drift-vs-conflict slope did not include the true slope value—again indicating biased estimation. In no case did the HDDM yield a near-zero bias across all conditions: some conflict level’s drift was always significantly mis-estimated. These consistent results reinforce that the HDDM’s failure was not a one-off fluke but a reliable outcome given the structural omission of a norm term. !Behavioral Signatures Figure 5: Behavioral signatures showing the relationship between error rates, response times, and conflict levels for different values of $w_n$. !HDDM Analysis Figure 6: Comparison of parameter recovery between NES and HDDM models. Behavioral Signatures: The main text references Figure 3 (as an example) for the pattern of RT and error vs. conflict, and describes a joint analysis. For clarity, Figure 4 (panel A) plots error rate as a function of $\lambda$ for three simulated conditions: a “low $w_n$” individual ($w_n = 0.2$), a “medium $w_n$” ($w_n = 1.0$), and a “high $w_n$” ($w_n = 2.0$), all with the same $w_s$ and other parameters. The low $w_n$ curve shows increasing errors with more conflict (typical DDM behavior), whereas the high $w_n$ curve shows decreasing errors (the normative dominance effect). Panel B of Figure 4 shows the mean correct RT for those same conditions: low $w_n$ leads to slowing with conflict, high $w_n$ leads to slight speeding up or at least no slowing at high conflict. Figure 5 in the main text combined these into a single scatter: we plotted the slope of RT (y-axis) against the slope of error rate (x-axis) across $\lambda \in [0,1]$ for a range of $w_n$ values and also for some alternative models (e.g., a DDM with threshold varying by condition). The “high $w_n$” points occupied the quadrant of negative error slope, negative RT slope, where none of the standard model points appeared (Bello & Malle, 2023; Cushman, 2015). This visualization underlines the uniqueness of the NES predictions. We include reproductions of these figures in the supplementary materials for reference. Summary of Phase Results: For convenient reference, we summarize the three testing phases and their outcomes (as listed in the bullet points at the end of Section 3 in the main text):
Identifiability Proof: Both ABC-SMC and NPE yielded approximately uniform SBC rank histograms for $w_n$ (and other parameters), confirming well-calibrated recovery of the norm weight (Bello & Malle, 2023).
Architectural Distinctness: The hierarchical DDM (HDDM) exhibited systematic failures (bias and miscalibration) in recovering the norm influence, underscoring that a model lacking an explicit $w_n$ cannot correctly account for norm-driven dynamics [@Hare2009SelfControl].
Distinct Behavioral Patterns: NES produced characteristic RT and accuracy signatures under graded conflict that only emerged with a non-zero $w_n$; alternative models could not replicate these joint patterns, indicating no equifinal solution without the normative parameter (Bello & Malle, 2023).
These points, taken together, form the crux of our argument that normative influence is an identifiable and functionally distinct component in the decision process as implemented in NES.