<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jesse Wright" />
  <meta name="dcterms.date" content="2025-05-08" />
  <title>Normative Executive System (NES): A Proof-of-Concept Computational Testbed for Identifiable Normative Influence</title>
  <style>
    html {
      font-size: 11pt;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  
  
  
  
  
  
  
  
</head>
<body>
<header id="title-block-header">
<h1 class="title">Normative Executive System (NES): A Proof-of-Concept
Computational Testbed for Identifiable Normative Influence</h1>
<p class="author">Jesse Wright</p>
<p class="date">May 8, 2025</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<p>We propose that normative influence in human decision-making is
computationally primitive—not emergent from utility maximization,
emotional response, or social learning—but a fundamental architectural
component of moral cognition. To test this theoretical claim, we
introduce the Normative Executive System (NES), extending the Drift
Diffusion Model with an explicit norm weight parameter (<span
class="math inline"><em>w</em><sub><em>n</em></sub></span>) that
modulates decision dynamics independently of utility-based signals. Our
empirical test employs Simulation-Based Calibration across multiple
inference methods (ABC-SMC and Neural Posterior Estimation), revealing
the key finding that <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> is
statistically identifiable from behavioral data, suggesting normative
influence leaves distinct computational signatures unexplained by
existing models. A critical result emerged from standard hierarchical
DDMs, which—even when enhanced with regression over conflict
conditions—systematically failed to recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> from
NES-generated data, highlighting representational limits under NES-like
task conditions. These results provide the first direct computational
evidence that moral decision-making benefits from explicit normative
representation, establishing a formal dissociation between normative and
general value-based choice models. The findings suggest that normative
self-governance is not merely an emergent byproduct, but a measurable
and computationally distinct faculty, offering new pathways for
understanding moral cognition.</p>
<p><em>Keywords:</em> Normative Executive System; Moral Cognition; Drift
Diffusion Model; Simulation-Based Inference; Norm Weight; Decision
Neuroscience</p>
</div>
</header>
<h1 id="norm-governed-decisions-a-computational-puzzle">1. Norm-Governed
Decisions: A Computational Puzzle</h1>
<p>People routinely make norm-consistent choices even when payoff,
emotion, and cognitive control all favor defection. We stop at red
lights on empty roads, keep unenforceable promises, and help strangers
at our own expense. These behaviors persist even when material
incentives or learned associations would push us the other
way—suggesting an internal mechanism that cannot be reduced to reward,
affect, or generic control.</p>
<p>Most formal models of decision-making subsume norms under emergent
processes within general-purpose architectures:</p>
<ul>
<li><strong>Utility models</strong> treat norms as high-value
preferences<br />
</li>
<li><strong>Learning theories</strong> invoke conditioned associations
via social feedback<br />
</li>
<li><strong>Dual-process accounts</strong> emphasize emotional
inhibition or deliberative override<br />
</li>
<li><strong>Conflict-monitoring frameworks</strong> cast restraint as
domain-general control</li>
</ul>
<p>Under these views, norm-adherence arises “for free” as a byproduct of
utility maximization, social learning, or inhibitory control. Yet many
moral choices strike us as uncued, unsupervised, and unmotivated by
self-interest—prompting a different question:</p>
<blockquote>
<p><em>Could normative influence itself be a primitive component of the
decision architecture?</em></p>
</blockquote>
<p>To explore this, we introduce the <strong>Normative Executive System
(NES)</strong>, a minimal extension of the Drift Diffusion Model (DDM)
that encodes an explicit <strong>norm weight</strong> (<span
class="math inline"><em>w</em><sub><em>n</em></sub></span>) alongside
standard salience and threshold parameters. By embedding <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> directly into
the drift equation, NES operationalizes the hypothesis that norms exert
a distinct, recoverable pull on evidence accumulation—rather than
emerging from other model dimensions.</p>
<p>Recent reviews highlight a lack of empirical tests for whether
explicit normative representations are necessary to capture moral
behavior <span class="citation"
data-cites="bello2023computationalapproachesto cushman2015moralconstraints">[@bello2023computationalapproachesto;
@cushman2015moralconstraints]</span>. NES targets this gap by using
<strong>Simulation-Based Calibration</strong> (ABC-SMC and Neural
Posterior Estimation) to ask three questions:</p>
<ol type="1">
<li><strong>Identifiability:</strong> Is <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> statistically
recoverable from simulated behavior?<br />
</li>
<li><strong>Distinctiveness:</strong> Does NES generate decision
patterns that standard DDMs cannot reproduce by parameter tuning
alone?<br />
</li>
<li><strong>Structural Mismatch:</strong> Do fortified hierarchical DDMs
systematically fail to recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> from
NES-generated data?</li>
</ol>
<p>By shifting the debate from philosophical intuition to statistical
identifiability and predictive utility, NES provides a formal, testable
framework for investigating whether normative self-governance is a
measurable, architecturally distinct faculty in human
decision-making.</p>
<h2 id="related-work">1.1 Related Work</h2>
<p>NES builds on a rich tradition of cognitive modeling, including the
conflict monitoring framework <span class="citation"
data-cites="Botvinick2001ConflictMonitoring">[@Botvinick2001ConflictMonitoring]</span>,
dual-process theories of moral judgment <span class="citation"
data-cites="Greene2001fMRI">[@Greene2001fMRI]</span>, and attribute-wise
value integration models <span class="citation"
data-cites="Hare2009SelfControl">[@Hare2009SelfControl]</span>. However,
these models either lack explicit norm representations or do not
validate the identifiability of such constructs from behavior. NES
proposes a principled extension of the Drift Diffusion Model that treats
norm influence as an independent signal—recoverable and separable from
salience or utility.</p>
<h1
id="the-normative-executive-system-formalizing-primitive-moral-architecture">2.
The Normative Executive System: Formalizing Primitive Moral
Architecture</h1>
<h2 id="core-architectural-principle">2.1 Core Architectural
Principle</h2>
<p>The <strong>Normative Executive System (NES)</strong> extends the
classic Drift Diffusion Model by adding a dedicated <strong>norm
weight</strong> (<span
class="math inline"><em>w</em><sub><em>n</em></sub></span>) alongside
the standard salience weight (<span
class="math inline"><em>w</em><sub><em>s</em></sub></span>). The drift
rate is defined as</p>
<p><span
class="math display"><em>v</em> = <em>w</em><sub><em>s</em></sub> (1 − <em>λ</em>) − <em>w</em><sub><em>n</em></sub> <em>λ</em></span></p>
<p>where <span class="math inline"><em>λ</em> ∈ [0, 1]</span> indexes
conflict between stimulus salience and internalized norms. Setting <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> apart as its
own parameter creates an <strong>oppositional architecture</strong> in
which norms exert a direct, quantifiable pull on evidence accumulation
rather than being folded into existing utility or control signals.</p>
<h2 id="why-this-tests-computational-primitiveness">2.2 Why This Tests
Computational Primitiveness</h2>
<p>This simple formulation makes three critical tests possible:</p>
<ol type="1">
<li><strong>Oppositional Architecture:</strong> By pitting <span
class="math inline"><em>w</em><sub><em>s</em></sub></span> vs. <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> in direct
competition, we force the model to resolve norm–salience conflict
explicitly.<br />
</li>
<li><strong>Parameter Independence:</strong> Because <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> is not a
transform of other variables, its recoverability signals a true
architectural primitive.<br />
</li>
<li><strong>Graded Conflict:</strong> Continuous variation in <span
class="math inline"><em>λ</em></span> lets us probe whether the model
responds systematically across low to high conflict—a hallmark of
dedicated processing.</li>
</ol>
<h2 id="architectural-predictions">2.3 Architectural Predictions</h2>
<p>NES yields three testable predictions that distinguish it from
emergentist accounts:</p>
<ol type="1">
<li><strong>Behavioral Signatures:</strong> Unique reaction-time and
error-rate patterns (e.g., non-monotonic RT under high <span
class="math inline"><em>λ</em></span>, systematic response suppression)
that standard DDMs cannot mimic.<br />
</li>
<li><strong>Statistical Identifiability:</strong> The norm weight <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> should be
recoverable from choice/RT data with minimal ambiguity.<br />
</li>
<li><strong>Architectural Irreducibility:</strong> Models lacking an
explicit <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> should
systematically fail to capture key norm-driven dynamics when fit to
NES-generated data.</li>
</ol>
<h2 id="simulation-inference-overview">2.4 Simulation &amp; Inference
Overview</h2>
<p>To evaluate these predictions, we implemented NES in a standard DDM
framework (see Supplement A for full details). Briefly:</p>
<ul>
<li><strong>Task:</strong> A parametric “Stroop-like” conflict paradigm
with five <span class="math inline"><em>λ</em></span> levels and
moderate trial counts to test identifiability under realistic session
lengths.<br />
</li>
<li><strong>Simulation:</strong> Euler–Maruyama integration generates
choice/RT data under known <span
class="math inline">(<em>w</em><sub><em>s</em></sub>, <em>w</em><sub><em>n</em></sub>, <em>λ</em>)</span>
settings.<br />
</li>
<li><strong>Inference:</strong> We apply both <strong>ABC-SMC</strong>
and <strong>Neural Posterior Estimation (NPE)</strong> to assess
recovery of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> (Supplement
B).<br />
</li>
<li><strong>Benchmark:</strong> A fortified hierarchical HDDM model—with
per-condition drift regressors—tests whether standard approaches can
indirectly recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>.</li>
</ul>
<h2 id="key-validation-steps">2.5 Key Validation Steps</h2>
<ol type="1">
<li><strong>Identifiability:</strong> SBC rank‐histograms for both
ABC-SMC and NPE should approximate uniformity, indicating
well-calibrated recovery of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>.<br />
</li>
<li><strong>HDDM Failure:</strong> If HDDM’s drift regressions produce
biased or uncalibrated <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> estimates,
this confirms an architectural mismatch.<br />
</li>
<li><strong>Behavioral Signatures:</strong> Simulated RT/error curves
under varying <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> should reveal
patterns (e.g., conflict-conditioned suppression) unique to NES.</li>
</ol>
<p>By keeping the core drift equation and predictions
front-and-center—and delegating algorithmic details, parameter settings,
and summary‐statistic choices to the Supplement—this section emphasizes
NES’s conceptual contribution while reserving technical depth for
specialist readers.</p>
<h2 id="hddm-recovery-of-w_n">2.6 HDDM Recovery of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span></h2>
<h3 id="pipeline-validation-standard-ddm-data">2.5.1 Pipeline Validation
(Standard DDM Data)</h3>
<p>Before analyzing NES data, we first validated our HDDM implementation
on standard DDM-simulated data. Five synthetic subjects each completed
3000 trials generated with true values of <span
class="math inline"><em>a</em> = 1.5</span>, <span
class="math inline"><em>v</em> = 0.5</span>, and <span
class="math inline"><em>t</em><sub>0</sub> = 0.2</span>. The recovered
posterior distributions are shown in Figure 1.</p>
<p>All parameters were accurately recovered, with posterior mass
concentrated around ground truth values and all estimates falling within
5% of their respective true values. This confirms that the HDDM stack is
functioning as expected under its own assumptions.</p>
<figure id="fig:hddm_sanity">
<img src="figures/hddm_sanity_check.png" style="width:90.0%"
alt="Posterior distributions for a, v, and t_0 recovered by HDDM on standard DDM-simulated data." />
<figcaption aria-hidden="true">Posterior distributions for <span
class="math inline"><em>a</em></span>, <span
class="math inline"><em>v</em></span>, and <span
class="math inline"><em>t</em><sub>0</sub></span> recovered by HDDM on
standard DDM-simulated data.</figcaption>
</figure>
<h3 id="fortified-hddm-on-nes-data">2.5.2 Fortified HDDM on NES
Data</h3>
<p>We next tested whether HDDM could indirectly recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> when applied
to NES-simulated data. To give HDDM every possible advantage, we
implemented a fortified hierarchical model with per-condition drift
regressors. The setup was as follows:</p>
<ul>
<li><strong>True values</strong>: <span
class="math inline"><em>w</em><sub><em>n</em></sub> ∼ Uniform(0.1, 2.0)</span><br />
</li>
<li><strong>Simulated data</strong>: NES-DDM with 5 subjects × 1000
trials; 5 <span class="math inline"><em>λ</em></span> levels<br />
</li>
<li><strong>Fit model</strong>: HDDM with group-level estimates of <span
class="math inline"><em>a</em></span>, <span
class="math inline"><em>t</em><sub>0</sub></span>, and <span
class="math inline"><em>v</em>(<em>λ</em>)</span><br />
</li>
<li><strong>Derived <span
class="math inline"><em>w</em><sub><em>n</em></sub></span></strong>:
slope from regression of drift rates on conflict<br />
</li>
<li><strong>SBC</strong>: ranks of inferred <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> vs. ground
truth</li>
</ul>
<p>The results were unambiguous: - <strong>Parameter recovery</strong>
was poor: Pearson <span
class="math inline"><em>r</em> = 0.29</span>–0.62, <span
class="math inline"><em>R</em><sup>2</sup> = 0.05</span>–0.38<br />
- <strong>Systematic bias</strong> was observed (e.g., <span
class="math inline"><em>v</em><sub>0.5</sub></span> bias = −0.48)<br />
- <strong>Coverage within ±0.1</strong> was only 21%–35%<br />
- <strong>SBC ranks</strong> were maxed out (rank = 1000), indicating
severe overconfidence and miscalibration</p>
<figure id="fig:fortified_chart">
<img src="figures/fortified_hddm_chart.png" style="width:90.0%"
alt="Comparison of true vs. estimated drift rates across conflict levels from HDDM applied to NES-simulated data. Each point represents one SBC iteration (subject-averaged group estimate). Systematic deviation from the identity line indicates consistent misrecovery of drift rate dynamics due to architectural mismatch." />
<figcaption aria-hidden="true">Comparison of true vs. estimated drift
rates across conflict levels from HDDM applied to NES-simulated data.
Each point represents one SBC iteration (subject-averaged group
estimate). Systematic deviation from the identity line indicates
consistent misrecovery of drift rate dynamics due to architectural
mismatch.</figcaption>
</figure>
<figure id="fig:hddm_hist">
<img src="figures/fortified_hddm_histogram.png" style="width:80.0%"
alt="Histogram of estimation error (estimated – true) for each drift coefficient. Biases are visible in most conflict levels, especially at λ = 0.5 and λ = 1.0. This structured error pattern confirms that HDDM cannot represent the principled normative effects encoded by NES." />
<figcaption aria-hidden="true">Histogram of estimation error (estimated
– true) for each drift coefficient. Biases are visible in most conflict
levels, especially at λ = 0.5 and λ = 1.0. This structured error pattern
confirms that HDDM cannot represent the principled normative effects
encoded by NES.</figcaption>
</figure>
<table style="width:100%;">
<colgroup>
<col style="width: 36%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr>
<th>Parameter</th>
<th><span class="math inline"><em>r</em></span></th>
<th><span class="math inline"><em>R</em><sup>2</sup></span></th>
<th>Bias</th>
<th>Std Err</th>
<th>±0.1 Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td><span
class="math inline"><em>v</em><sub><em>I</em><em>n</em><em>t</em><em>e</em><em>r</em><em>c</em><em>e</em><em>p</em><em>t</em></sub></span></td>
<td>0.62</td>
<td>0.38</td>
<td>-0.15</td>
<td>0.28</td>
<td>0.35</td>
</tr>
<tr>
<td><span class="math inline"><em>v</em><sub>[0.25]</sub></span></td>
<td>0.57</td>
<td>0.32</td>
<td>-0.22</td>
<td>0.30</td>
<td>0.30</td>
</tr>
<tr>
<td><span class="math inline"><em>v</em><sub>[0.5]</sub></span></td>
<td>0.42</td>
<td>0.18</td>
<td>-0.48</td>
<td>0.44</td>
<td>0.25</td>
</tr>
<tr>
<td><span class="math inline"><em>v</em><sub>[0.75]</sub></span></td>
<td>0.38</td>
<td>0.15</td>
<td>-0.31</td>
<td>0.33</td>
<td>0.23</td>
</tr>
<tr>
<td><span class="math inline"><em>v</em><sub>[1.0]</sub></span></td>
<td>0.29</td>
<td>0.05</td>
<td>-0.27</td>
<td>0.37</td>
<td>0.21</td>
</tr>
</tbody>
</table>
<p><strong>Table 1:</strong> Drift rate regression recovery metrics from
HDDM applied to NES-simulated data (20 SBC iterations).</p>
<p>HDDM fails not due to noise, but because it lacks an architectural
slot for norm weighting—treating drift rate variations as unstructured.
This mismatch explains its systematic failure to recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> from
NES-simulated data.</p>
<h2 id="neural-posterior-estimation-npe-for-multi-parameter-sbc">2.7
Neural Posterior Estimation (NPE) for Multi-Parameter SBC</h2>
<p>To assess the joint identifiability of core Minimal NES parameters
and to leverage potentially more efficient inference, we conducted
Simulation-Based Calibration (SBC) using Neural Posterior Estimation
(NPE). This approach aimed to recover effective norm weight (<span
class="math inline"><em>w</em><sub><em>n</em>_<em>e</em><em>f</em><em>f</em></sub></span>),
threshold (<span class="math inline"><em>a</em></span>), non-decision
time (<span class="math inline"><em>t</em></span>), and effective
salience (<span
class="math inline"><em>w</em><sub><em>s</em></sub></span>).</p>
<p>The inference stack and SBC procedure were as follows:</p>
<ul>
<li><strong>Parameters of Interest &amp; Priors</strong>: For each SBC
iteration <span class="math inline"><em>i</em></span>, a true parameter
vector <span
class="math inline"><em>θ</em><sub><em>t</em><em>r</em><em>u</em><em>e</em></sub><sup>(<em>i</em>)</sup> = (<em>w</em><sub><em>n</em>_<em>e</em><em>f</em><em>f</em>, <em>t</em><em>r</em><em>u</em><em>e</em></sub><sup>(<em>i</em>)</sup>, <em>a</em><sub><em>t</em><em>r</em><em>u</em><em>e</em></sub><sup>(<em>i</em>)</sup>, <em>t</em><sub><em>t</em><em>r</em><em>u</em><em>e</em></sub><sup>(<em>i</em>)</sup>, <em>w</em><sub><em>s</em>, <em>t</em><em>r</em><em>u</em><em>e</em></sub><sup>(<em>i</em>)</sup>)</span>
was drawn from a joint prior. Based on pilot recovery studies
(<code>run_parameter_recovery_minimal_nes_npe.py</code>), these were:
<ul>
<li><span
class="math inline"><em>w</em><sub><em>n</em>_<em>e</em><em>f</em><em>f</em></sub> ∼ Uniform(0.1, 2.0)</span></li>
<li><span class="math inline"><em>a</em> ∼ Uniform(0.4, 1.5)</span></li>
<li><span
class="math inline"><em>t</em> ∼ Uniform(0.05, 0.5)</span></li>
<li><span
class="math inline"><em>w</em><sub><em>s</em></sub> ∼ Uniform(0.2, 1.5)</span></li>
</ul></li>
<li><strong>Simulator</strong>: The NES-derived DDM (as per Section 2.1,
with <span class="math inline"><em>σ</em> = 1.0</span> and <span
class="math inline"><em>T</em><sub><em>m</em><em>a</em><em>x</em></sub> = 10.0<em>s</em></span>
from <code>BASE_SIM_PARAMS_RECOVERY</code> used in
<code>run_parameter_recovery_minimal_nes_npe.py</code>) generating
choice/RT for the 5-level Stroop-like task (as per Section 2.2, <span
class="math inline"><em>N</em> = 300</span> trials).</li>
<li><strong>Fixed DDM Simulation Constants</strong>: Noise <span
class="math inline"><em>σ</em> = 1.0</span>, <span
class="math inline"><em>d</em><em>t</em> = 0.01<em>s</em></span>, <span
class="math inline"><em>T</em><sub><em>m</em><em>a</em><em>x</em></sub> = 10.0<em>s</em></span>.
The parameters <span
class="math inline"><em>w</em><sub><em>n</em></sub>_<em>e</em><em>f</em><em>f</em>, <em>a</em>, <em>t</em>, <em>w</em><sub><em>s</em></sub></span>
were the free parameters drawn from priors.</li>
<li><strong>Summary Statistics</strong>: The same comprehensive set of
conditional and normalized summary statistics as described in Section
2.3 were used as input to the NPE.</li>
<li><strong>Inference Method</strong>: Neural Posterior Estimation
(NPE), specifically SNPE-C (which uses Masked Autoregressive Flows as
the density estimator), implemented via the <code>sbi</code> Python
package <span class="citation"
data-cites="sbi_package_tejero_etal_2020 sbi_package_cranmer_etal_2020">[@sbi_package_tejero_etal_2020;
@sbi_package_cranmer_etal_2020]</span>.</li>
<li><strong>NPE Training</strong>: A single NPE model was trained once
on <span
class="math inline"><em>N</em><sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em>_<em>s</em><em>i</em><em>m</em><em>s</em></sub> = 10, 000</span>
simulations. Each simulation involved drawing a parameter vector <span
class="math inline"><em>θ</em></span> from the joint prior and
generating a corresponding vector of summary statistics <span
class="math inline"><em>x</em></span>. The NPE was trained on these
<span class="math inline">(<em>θ</em>, <em>x</em>)</span> pairs.
Training converged successfully after approximately
[Avg_Epochs_NPE_Final_Run] epochs.</li>
<li><strong>Posterior Sampling</strong>: For each of the <span
class="math inline"><em>N</em><sub><em>S</em><em>B</em><em>C</em></sub> = 100</span>
“observed” datasets in the SBC loop, <span
class="math inline"><em>N</em><sub><em>p</em><em>o</em><em>s</em><em>t</em>_<em>s</em><em>a</em><em>m</em><em>p</em><em>l</em><em>e</em><em>s</em></sub> = 1000</span>
samples were drawn from the trained NPE posterior <span
class="math inline"><em>p</em>(<em>θ</em>|<em>x</em><sub><em>o</em><em>b</em><em>s</em></sub>)</span>.</li>
<li><strong>Rank Calculation</strong>: For each SBC iteration <span
class="math inline"><em>i</em></span> and for each of the four
parameters <span
class="math inline"><em>k</em> ∈ {<em>w</em><sub><em>n</em>_<em>e</em><em>f</em><em>f</em></sub>, <em>a</em>, <em>t</em>, <em>w</em><sub><em>s</em></sub>}</span>,
the rank was computed as: <span class="math display">$$
\mathrm{rank}^{(i)}_k = \sum_{j=1}^{1000}
\mathbf{1}\left(\theta_{k,\mathrm{post}}^{(i,j)} \leq
\theta_{k,\mathrm{true}}^{(i)}\right)
$$</span></li>
</ul>
<figure id="fig:npe_sbc">
<img src="figures/NPE_SBC_ECDF_6_Param.png" style="width:100.0%"
alt="Simulation-Based Calibration (SBC) ECDFs for NES parameters using Neural Posterior Estimation (NPE). Each panel shows the empirical cumulative distribution function (ECDF) of posterior ranks (blue), bounded by the 95% beta confidence interval for uniformity (red dashed). The diagonal line represents ideal calibration. All parameters show approximately uniform rank distributions, with w_n, w_s, and t_0 exhibiting especially strong calibration. These results confirm the joint identifiability and inferential precision of the NES model under NPE." />
<figcaption aria-hidden="true">Simulation-Based Calibration (SBC) ECDFs
for NES parameters using Neural Posterior Estimation (NPE). Each panel
shows the empirical cumulative distribution function (ECDF) of posterior
ranks (blue), bounded by the 95% beta confidence interval for uniformity
(red dashed). The diagonal line represents ideal calibration. All
parameters show approximately uniform rank distributions, with <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>, <span
class="math inline"><em>w</em><sub><em>s</em></sub></span>, and <span
class="math inline"><em>t</em><sub>0</sub></span> exhibiting especially
strong calibration. These results confirm the joint identifiability and
inferential precision of the NES model under NPE.</figcaption>
</figure>
<h2 id="implementation-details">2.8 Implementation Details</h2>
<p>All simulations and inference procedures were run on a workstation
with 40GB RAM and an NVIDIA RTX GPU. NES simulations used Python 3.10,
NumPy, and custom Euler–Maruyama integration. ABC-SMC was implemented
via pyABC <span class="citation"
data-cites="Klinger2018pyABC">[@Klinger2018pyABC]</span>. Neural
Posterior Estimation (NPE) used the sbi library <span class="citation"
data-cites="sbi_package_tejero_etal_2020">[@sbi_package_tejero_etal_2020]</span>,
with SNPE-C and masked autoregressive flows trained for ~120 epochs
using Adam (lr=1e-4). Code for simulations, summary statistic
extraction, and inference will be made available upon request or upon
publication.</p>
<h1
id="testing-computational-primitiveness-through-parameter-recovery">3.
Testing Computational Primitiveness Through Parameter Recovery</h1>
<h2 id="methodological-strategy">3.1 Methodological Strategy</h2>
<p>Our approach to testing computational primitiveness proceeds through
three phases, each designed to provide converging evidence for the
primitive nature of normative influence:</p>
<ol type="1">
<li><strong>Phase 1: Demonstrate <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>
identifiability using simulator-based inference</strong> (ABC-SMC, NPE)
<ul>
<li>Tests whether normative influence can be reliably recovered from
behavioral data</li>
<li>Uses multiple inference methods to ensure robustness</li>
<li>Assesses parameter recovery under realistic conditions</li>
</ul></li>
<li><strong>Phase 2: Show systematic failure of hierarchical DDM
approaches</strong>
<ul>
<li>Demonstrates that standard models cannot capture norm-driven
behavior</li>
<li>Highlights the architectural mismatch between emergentist and
primitive accounts</li>
<li>Provides negative evidence against purely emergentist
explanations</li>
</ul></li>
<li><strong>Phase 3: Characterize unique behavioral signatures of
normative influence</strong>
<ul>
<li>Identifies patterns specific to norm-driven behavior</li>
<li>Shows these patterns cannot be mimicked by utility or control
parameters</li>
<li>Provides positive evidence for the primitive nature of normative
influence</li>
</ul></li>
</ol>
<p>This progression directly tests the core prediction that if norms are
computationally primitive, they should be identifiable through
appropriate methods but invisible to methods that assume an emergent
architecture. The following sections present the results of each phase,
with methodological details provided in the corresponding sections.</p>
<h2 id="the-logic-of-simulation-based-calibration">3.2 The Logic of
Simulation-Based Calibration</h2>
<p>To test whether normative influence is computationally primitive, we
must demonstrate that:</p>
<ol type="1">
<li><strong>NES parameters are identifiable</strong>: If <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> reflects real
computational processes, it should be recoverable from behavioral
data</li>
<li><strong>Standard models fail systematically</strong>: If norms are
primitive (not emergent), existing frameworks should show architectural
mismatch when applied to norm-driven data</li>
<li><strong>Behavioral signatures are unique</strong>: Normative
influence should produce patterns that utility/control models cannot
mimic</li>
</ol>
<p>Simulation-Based Calibration (SBC) provides the ideal framework for
testing these claims under controlled conditions.</p>
<h2 id="phase-1-identifiability-via-simulator-based-inference">3.3 Phase
1: Identifiability via Simulator-Based Inference</h2>
<h3 id="abc-smc-recovery-of-w_n">ABC-SMC Recovery of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span></h3>
<p>We first assessed the identifiability of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> using
Approximate Bayesian Computation with Sequential Monte Carlo (ABC-SMC).
Across 100 simulated datasets with 150 posterior samples each, we
observed excellent calibration of the posterior estimates, with the rank
histogram closely approximating uniformity (<span
class="math inline"><em>χ</em><sup>2</sup></span>(14) = 22.95, <span
class="math inline"><em>p</em> = 0.061</span>). This indicates that the
ABC-SMC pipeline yields well-calibrated posterior estimates without
systematic bias.</p>
<figure id="fig:rank">
<img src="figures/sbc_rank_histogram.png" style="width:80.0%"
alt="SBC rank histogram for w_n, showing approximately uniform rank distribution across 100 iterations." />
<figcaption aria-hidden="true">SBC rank histogram for <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>, showing
approximately uniform rank distribution across 100
iterations.</figcaption>
</figure>
<h3 id="neural-posterior-estimation-npe-results">Neural Posterior
Estimation (NPE) Results</h3>
<p>To complement the ABC-SMC approach and address its limitations, we
implemented Neural Posterior Estimation (NPE) for multi-parameter
recovery. The NPE model was trained on 10,000 simulations and showed
robust recovery of all parameters, including <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>. The joint
posterior distributions demonstrated clear separation between
parameters, indicating that <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> is
identifiable even when other parameters are free to vary.</p>
<h3 id="robustness-under-parameter-jitter">Robustness Under Parameter
Jitter</h3>
<p>To assess identifiability under realistic uncertainty, we introduced
±10% uniform jitter to fixed parameters (<span
class="math inline"><em>a</em></span>, <span
class="math inline"><em>t</em><sub>0</sub></span>, <span
class="math inline"><em>w</em><sub><em>s</em></sub></span>) across 50
SBC runs. Results confirmed that <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> remains
identifiable under these conditions, with median rank deviation &lt; 5%
and stable 95% coverage. This robustness supports NES’s applicability in
settings with individual variation.</p>
<h2 id="phase-2-architectural-failure-of-hddm">3.4 Phase 2:
Architectural Failure of HDDM</h2>
<p>The failure of standard hierarchical DDMs (HDDMs) to recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> provides
strong evidence for the architectural distinctness of normative
influence. When we applied HDDM with regression over conflict levels to
NES-generated data, we observed systematic failures in parameter
recovery:</p>
<ul>
<li><strong>Poor parameter recovery</strong>: Pearson <span
class="math inline"><em>r</em></span> = 0.29–0.62, <span
class="math inline"><em>R</em><sup>2</sup></span> = 0.05–0.38</li>
<li><strong>Systematic bias</strong>: Consistent underestimation of
drift rates across conflict levels</li>
<li><strong>Miscalibrated uncertainty</strong>: SBC ranks were
consistently at maximum, indicating systematic overconfidence</li>
</ul>
<figure id="fig:hddm_hist">
<img src="figures/fortified_hddm_histogram.png" style="width:80.0%"
alt="HDDM rank histogram for w_n estimates. Ranks clustered at the maximum value indicate systematic miscalibration, reflecting HDDM’s inability to capture the normative component." />
<figcaption aria-hidden="true">HDDM rank histogram for <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> estimates.
Ranks clustered at the maximum value indicate systematic miscalibration,
reflecting HDDM’s inability to capture the normative
component.</figcaption>
</figure>
<p>This failure is not due to insufficient data or model flexibility,
but rather reflects a fundamental architectural mismatch—HDDM lacks the
representational capacity to capture the normative gating mechanism
implemented in NES.</p>
<h2 id="phase-3-unique-behavioral-signatures-of-w_n">3.5 Phase 3: Unique
Behavioral Signatures of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span></h2>
<h3 id="distinct-response-patterns">Distinct Response Patterns</h3>
<p>High values of <em>w</em>_n produce a characteristic pattern of
decreasing RTs and error rates with increasing conflict—a signature that
cannot be explained by standard decision variables. Figure 4 shows how
different values of <em>w</em>_n lead to distinct behavioral profiles
across conflict levels.</p>
<figure id="fig:wn_signature">
<img src="figures/wn_behavioral_signature.png" style="width:80.0%"
alt="Behavioral signatures of the norm weight w_n across conflict levels. Left: Error rate decreases more steeply with conflict for higher w_n, indicating stronger suppression of salience-driven responses. Right: Mean correct response times (RTs) also decrease with higher w_n, suggesting faster commitment in norm-congruent decisions. These patterns are not replicable by standard DDMs and reflect the architectural distinctiveness of NES." />
<figcaption aria-hidden="true">Behavioral signatures of the norm weight
<span class="math inline"><em>w</em><sub><em>n</em></sub></span> across
conflict levels. Left: Error rate decreases more steeply with conflict
for higher <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>, indicating
stronger suppression of salience-driven responses. Right: Mean correct
response times (RTs) also decrease with higher <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>, suggesting
faster commitment in norm-congruent decisions. These patterns are not
replicable by standard DDMs and reflect the architectural
distinctiveness of NES.</figcaption>
</figure>
<h3 id="conflict-conditioned-error-rates">Conflict-Conditioned Error
Rates</h3>
<p>A key prediction of the NES framework is that high <em>w</em>_n
values should lead to decreasing error rates at higher conflict levels—a
pattern not predicted by standard decision models. This prediction was
confirmed in our simulations (Figure 5).</p>
<figure id="fig:error_rates">
<img src="figures/Error_Rate_by_Conflict_level.png" style="width:80.0%"
alt="Left: Error rates by conflict level (\lambda) across five parameter regimes. Only the high_wn condition exhibits a strong monotonic suppression of errors with increased conflict. Right: Joint behavioral slope profiles (RT vs. error rate). The high_wn point lies in the lower-left quadrant, combining decreasing RTs and decreasing errors with conflict—a signature that no other parameter combination replicates. These results demonstrate the behavioral distinctiveness of w_n and reject equifinality from alternative DDM parameterizations." />
<figcaption aria-hidden="true">Left: Error rates by conflict level
(<span class="math inline"><em>λ</em></span>) across five parameter
regimes. Only the high_wn condition exhibits a strong monotonic
suppression of errors with increased conflict. Right: Joint behavioral
slope profiles (RT vs. error rate). The high_wn point lies in the
lower-left quadrant, combining decreasing RTs and decreasing errors with
conflict—a signature that no other parameter combination replicates.
These results demonstrate the behavioral distinctiveness of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> and reject
equifinality from alternative DDM parameterizations.</figcaption>
</figure>
<h3 id="equifinality-analysis">Equifinality Analysis</h3>
<p>We tested whether combinations of other parameters could mimic the
effects of <em>w</em>_n by simulating five distinct parameter
regimes:</p>
<ul>
<li><strong>Low <em>w</em>_n (0.2)</strong>: impulsive, error-prone
under high conflict</li>
<li><strong>Mid <em>w</em>_n (0.5–1.0)</strong>: balanced
adaptation</li>
<li><strong>High <em>w</em>_n (1.5–2.0)</strong>: slower,
norm-consistent responding</li>
</ul>
<p>These regimes produced distinct and reproducible patterns in RT and
accuracy that could not be matched by threshold or salience weight
alone, confirming that <em>w</em>_n captures a unique dimension of
behavioral variation.</p>
<ul>
<li><strong>Identifiability Proof:</strong> SBC rank‐histograms for both
ABC-SMC and NPE approximate uniformity, confirming well-calibrated
recovery of <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>.<br />
</li>
<li><strong>Architectural Distinctness:</strong> HDDM’s systematic
failure underscores that emergentist models cannot reproduce NES’s
normative gating.<br />
</li>
<li><strong>Distinct Behavioral Patterns:</strong> Only NES yields the
characteristic RT and accuracy signatures under graded conflict.</li>
</ul>
<p>Together, these results converge to show that normative weight
operates as a mechanistically distinct and recoverable component of
decision dynamics—supporting the thesis that normative influence is a
computational primitive rather than an emergent artifact of
general‐purpose models.</p>
<h1 id="early-pilot-evidence-for-human-framing-data">4. Early Pilot
Evidence for Human Framing Data</h1>
<p>As an exploratory proof-of-concept, we applied a 5-parameter NES
variant (including an <span
class="math inline"><em>α</em><sub>gain</sub></span> learning rate) to
archival framing-choice data from <span
class="math inline"><em>N</em> = 45</span> participants (modeled after
Roberts &amp; Gershman, 2021). Using the same NPE pipeline validated
above, we extracted each subject’s mean normative weight (<span
class="math inline"><em>w</em><sub><em>n</em></sub></span>) and
correlated it with their individual framing susceptibility (risk
preference difference between gain vs. loss frames).</p>
<blockquote>
<p><strong>Exploratory finding:</strong> Subjects with higher
NES-inferred <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> tended to
show a larger framing effect (<span
class="math inline"><em>r</em> = 0.87</span>, <span
class="math inline"><em>p</em> &lt; 0.0001</span>; see Fig. 7).</p>
</blockquote>
<p>Because this analysis was not pre-registered and lacks full
experimental details (participant exclusions, exact task timing, and
comprehensive model comparisons), we present it here as preliminary
“teaser” evidence. A dedicated empirical follow-up—complete with full
methods, hierarchical modeling, and formal model‐comparison metrics—is
forthcoming <span class="citation"
data-cites="WrightInPrep">[@WrightInPrep]</span>.</p>
<h1 id="discussion">5. Discussion</h1>
<h2 id="key-takeaways">5.1 Key Takeaways</h2>
<p>We provide the first simulation-based proof that a dedicated
<strong>norm weight</strong> (<span
class="math inline"><em>w</em><sub><em>n</em></sub></span>) is both
<strong>identifiable</strong> and <strong>functionally distinct</strong>
in decision architectures.<br />
- <strong>Calibrated Recovery:</strong> SBC via ABC-SMC and NPE yields
uniform rank histograms for <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>, confirming
reliable inference under realistic trial counts.<br />
- <strong>Unique Behavioral Patterns:</strong> Varying <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> produces
non-monotonic RT curves and conflict-conditioned suppression of
errors—signatures standard DDMs cannot replicate.<br />
- <strong>Architectural Gap:</strong> Fortified hierarchical HDDM
systematically misrecovers <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> (r=0.29–0.62,
R²&lt;0.4; extreme SBC skew), demonstrating that emergentist models lack
the structural slot for normative influence.</p>
<h2 id="implications-for-decision-modeling">5.2 Implications for
Decision Modeling</h2>
<ul>
<li><strong>Emergent vs. Primitive:</strong> These results challenge
views that norm-adherence “emerges” from value or control parameters
alone. Instead, explicit normative gating appears necessary to capture
moral dynamics.<br />
</li>
<li><strong>Necessity of Simulation-Based Inference:</strong>
Traditional DDM fitting fails where ABC-SMC and NPE succeed,
underscoring the importance of simulator-based methods when assessing
models with structured components.</li>
</ul>
<h2 id="mapping-nes-to-broader-theoretical-frameworks">5.3 Mapping NES
to Broader Theoretical Frameworks</h2>
<p>The Normative Executive System (NES) can be situated within classical
cognitive science frameworks, particularly Marr’s three levels of
analysis. This alignment helps clarify what NES contributes above and
beyond existing value-based models, and offers testable bridges between
normative theory and mechanistic modeling.</p>
<h3 id="computational-level-what-is-the-goal">Computational level (What
is the goal?)</h3>
<p>At this level, NES formalizes a distinct goal: to adhere to
internalized norms even when they conflict with salience or reward. The
norm weight parameter <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> quantifies
the agent’s commitment to this goal. This is conceptually akin to the
inclusion of “deontic value” in some RL models, but here it is treated
as a separable decision influence rather than a reweighted utility.</p>
<h3 id="algorithmic-level-how-is-it-computed">Algorithmic level (How is
it computed?)</h3>
<p>NES specifies a conflict-sensitive drift rate:</p>
<p><span
class="math display"><em>v</em> = <em>w</em><sub><em>s</em></sub>(1 − <em>λ</em>) − <em>w</em><sub><em>n</em></sub><em>λ</em></span></p>
<p>This oppositional formulation makes normative influence explicit and
measurable, unlike standard DDMs where such dynamics are implicit. It
resembles cognitive control mechanisms (e.g., dACC-driven conflict
monitoring), but instantiates them within a principled decision rule
that can be empirically validated.</p>
<h3
id="implementational-level-what-systems-realize-this">Implementational
level (What systems realize this?)</h3>
<p>NES offers testable neural predictions. Individuals with higher
inferred <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> may show
stronger mid-frontal theta activity during norm conflict, or distinct
modulation of value-related regions (e.g., vmPFC) and control systems
(e.g., dlPFC) depending on <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> and conflict
levels. This bridges NES to neurocognitive models of moral
cognition.</p>
<h2 id="nes-in-relation-to-integrated-value-based-accounts">5.4 NES in
Relation to Integrated Value-Based Accounts</h2>
<p>A prominent alternative to NES is the integrated value-based
framework, where norms are treated as weighted attributes within a
single accumulation process. Attribute-wise drift diffusion models
(anDDMs), such as those developed by Hutcherson et al., model decisions
as a function of competing value dimensions (e.g., hedonic
vs. normative), without positing a dedicated normative faculty. These
models replicate neural and behavioral data, including dlPFC activity
and norm-consistent choices, through attentional dynamics and weight
variation.</p>
<p>Similarly, Gautheron et al. demonstrate that moral behavior can
emerge from shared neural fields if normative evidence receives temporal
precedence or differential salience. These approaches show that a single
integrator, when tuned appropriately, can mimic normative behavior
without invoking architectural separation.</p>
<p>NES does not dispute the descriptive success of these models. Rather,
it highlights an inferential limitation: in tasks where norms conflict
directly with salience, unified DDMs—even when enhanced—systematically
fail to recover normative dynamics. This is evident in Section 2.5,
where HDDM was unable to recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> from
NES-simulated data (Figures 2–3), despite using a flexible regression
structure.</p>
<p>The key contribution of NES is not that it performs better on all
tasks, but that it isolates architectural necessity. If norms can be
modeled just by reweighting, then emergentist DDMs should be able to
recover <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>. That they do
not—despite extensive tuning—suggests that some normative processes may
demand explicit, dissociable representations. NES provides a framework
to test this claim with computational precision.</p>
<h2 id="methodological-notes">5.5 Methodological Notes</h2>
<ul>
<li><strong>SBC Scope:</strong> Although SBC uses NES-generated data, it
tests inferential calibration—not empirical validity. Full model-fit
diagnostics, convergence curves, and distance-metric choices are
detailed in Supplement A.<br />
</li>
<li><strong>Fixed Parameters:</strong> Piloted settings for <span
class="math inline"><em>a</em></span>, <span
class="math inline"><em>w</em><sub><em>s</em></sub></span>, and <span
class="math inline"><em>t</em><sub>0</sub></span> aided identifiability;
jitter analyses (±10%) confirm robustness, but hierarchical estimation
with constrained priors could enhance ecological validity.</li>
</ul>
<h2 id="practical-recommendations">5.6 Practical Recommendations</h2>
<ol type="1">
<li><strong>Design:</strong> Employ multiple, balanced conflict levels
and ≥1000 trials/participant.<br />
</li>
<li><strong>Inference:</strong> Use ABC-SMC or NPE rather than standard
HDDM.<br />
</li>
<li><strong>Validation:</strong> Always run SBC on each pipeline before
interpreting <span
class="math inline"><em>w</em><sub><em>n</em></sub></span>
estimates.</li>
</ol>
<h2 id="future-directions">5.7 Future Directions</h2>
<ul>
<li><strong>Hierarchical NES:</strong> Jointly estimate group and
individual <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> under
hierarchical priors.<br />
</li>
<li><strong>Neural Validation:</strong> Link <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> to
mid-frontal theta or fMRI markers of conflict.<br />
</li>
<li><strong>Applied Domains:</strong> Extend to clinical populations
(ASD, OCD) and developmental studies of norm acquisition.</li>
</ul>
<h2 id="conclusion">5.8 Conclusion</h2>
<p>By demonstrating that <span
class="math inline"><em>w</em><sub><em>n</em></sub></span> is
recoverable, produces unique behavioral signatures, and eludes standard
DDMs, we establish normative influence as a computational primitive. NES
lays the groundwork for treating concepts like duty and restraint as
measurable forces in moral cognition—a foundational advance in
computational moral theory.</p>
<p><em>Full technical details and extended analyses are provided in the
Supplement.</em></p>
<h1 id="data-and-code-availability">Data and Code Availability</h1>
<p>All code, preprocessed data, and containerized environments (Docker)
needed to reproduce every figure and analysis will be made publicly
available via [GitHub URL] and archived on Zenodo under DOI [Zenodo
DOI].</p>
<h1 id="appendix-a-summary-statistics">Appendix A: Summary
Statistics</h1>
<h2 id="a.1-complete-list-of-summary-statistics">A.1 Complete List of
Summary Statistics</h2>
<p>For each of the five conflict levels (<em>λ</em>), we computed the
following:</p>
<ul>
<li><strong>Error rate</strong></li>
<li><strong>Correct RTs</strong>:
<ul>
<li>Mean</li>
<li>Median</li>
<li>Variance</li>
<li>25th percentile</li>
<li>75th percentile</li>
<li>Skewness</li>
<li>Minimum</li>
<li>Maximum</li>
<li>Range</li>
</ul></li>
<li><strong>Error RTs</strong>: Same statistics as above</li>
</ul>
<p>All RT-based statistics were normalized by <span
class="math inline"><em>T</em><sub>max</sub> = 4.0</span> seconds to
maintain consistent scale across metrics.</p>
<h2 id="a.2-distance-function-weights">A.2 Distance Function
Weights</h2>
<p>We used a weighted L2 (Euclidean) distance metric across normalized
statistics, with weights chosen based on sensitivity to
<em>w</em>_n:</p>
<ul>
<li><strong>Error rate</strong>: 2.0 (highest priority)</li>
<li><strong>Correct RT mean &amp; median</strong>: 1.5 (high
priority)</li>
<li><strong>All other statistics</strong>: 1.0 (standard weight)</li>
</ul>
<p>These weights were selected heuristically based on preliminary runs
that prioritized behavioral features most impacted by norm weighting.
While not systematically optimized, sensitivity analyses suggested
robustness within a 1:1–2:1 weight ratio.</p>
<h2 id="a.3-data-quality-and-missing-values">A.3 Data Quality and
Missing Values</h2>
<p><strong>NaN Handling</strong>:</p>
<ul>
<li>If both simulated and observed values were NaN: the statistic was
excluded from the distance calculation.</li>
<li>If one value was NaN and the other was not: a penalty of
<strong>+100</strong> was added to the distance to strongly discourage
parameter regions producing undefined behavior (e.g., zero errors in a
high-conflict condition).</li>
</ul>
<p>This approach penalizes implausible parameter settings while
preserving numerical stability in the inference pipeline.</p>
<h2 id="appendix-b-glossary">Appendix B: Glossary</h2>
<ul>
<li><p><strong>DDM (Drift Diffusion Model)</strong>: A model of binary
decision-making using evidence accumulation.</p></li>
<li><p><strong>Drift rate (v)</strong>: The average rate at which
evidence accumulates toward a decision.</p></li>
<li><p><strong>Threshold (a)</strong>: The boundary distance that
determines how much evidence is needed before making a
decision.</p></li>
<li><p><strong>Non-decision time (t)</strong>: The time consumed by
processes other than decision-making (e.g., perception, motor
response).</p></li>
<li><p><strong>Simulation-Based Calibration (SBC)</strong>: A method to
assess whether posterior inference is well-calibrated when ground truth
is known.</p></li>
<li><p><strong>Neural Posterior Estimation (NPE)</strong>: A machine
learning technique for approximating posterior distributions without
handcrafted distance metrics.</p></li>
<li><p><strong>Norm Weight (wₙ)</strong>: A NES parameter representing
the strength of normative influence on decisions.</p></li>
</ul>
</body>
</html>
